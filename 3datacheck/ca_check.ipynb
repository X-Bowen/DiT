{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "731b4896-d9c6-4f72-87b3-908e78450bd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 18 .npz files in /scratch/bowenxi/dit/neural_tangent_kernel/feature_swin_b/ca/\n",
      "\n",
      "[1/18] Processing /scratch/bowenxi/dit/neural_tangent_kernel/feature_swin_b/ca/increment_1_train_tangent.npz\n",
      "\n",
      "Loading: /scratch/bowenxi/dit/neural_tangent_kernel/feature_swin_b/ca/increment_1_train_tangent.npz\n",
      "Features shape: (2305167, 1024), Labels shape: (2305167,)\n",
      "[NumPy] Loaded features -> min: -10.8768, max: 9.6011, mean: 0.0353\n",
      "[Torch] A = F^T F / n -> min: -20.4648, max: 21.8455, mean: 0.0013\n",
      "[Torch] F @ A -> min: -17043.8867, max: 15995.4365, mean: 56.5633\n",
      "[NumPy] F @ A (numpy) -> min: -17043.8867, max: 15995.4365, mean: 56.5628\n",
      "\n",
      "[2/18] Processing /scratch/bowenxi/dit/neural_tangent_kernel/feature_swin_b/ca/increment_1_val_tangent.npz\n",
      "\n",
      "Loading: /scratch/bowenxi/dit/neural_tangent_kernel/feature_swin_b/ca/increment_1_val_tangent.npz\n",
      "Features shape: (50000, 1024), Labels shape: (50000,)\n",
      "[NumPy] Loaded features -> min: -7.8972, max: 7.4992, mean: 0.0341\n",
      "[Torch] A = F^T F / n -> min: -19.6499, max: 21.0707, mean: 0.0012\n",
      "[Torch] F @ A -> min: -12339.3145, max: 11533.5195, mean: 52.3958\n",
      "[NumPy] F @ A (numpy) -> min: -12339.3145, max: 11533.5195, mean: 52.3957\n",
      "\n",
      "[3/18] Processing /scratch/bowenxi/dit/neural_tangent_kernel/feature_swin_b/ca/increment_2_train_tangent.npz\n",
      "\n",
      "Loading: /scratch/bowenxi/dit/neural_tangent_kernel/feature_swin_b/ca/increment_2_train_tangent.npz\n",
      "Features shape: (3329167, 1024), Labels shape: (3329167,)\n",
      "[NumPy] Loaded features -> min: -10.7076, max: 9.6138, mean: 0.0369\n",
      "[Torch] A = F^T F / n -> min: -20.8633, max: 22.0577, mean: 0.0014\n",
      "[Torch] F @ A -> min: -17222.4512, max: 16313.3613, mean: 60.4291\n",
      "[NumPy] F @ A (numpy) -> min: -17222.4512, max: 16313.3613, mean: 60.4303\n",
      "\n",
      "[4/18] Processing /scratch/bowenxi/dit/neural_tangent_kernel/feature_swin_b/ca/increment_2_val_tangent.npz\n",
      "\n",
      "Loading: /scratch/bowenxi/dit/neural_tangent_kernel/feature_swin_b/ca/increment_2_val_tangent.npz\n",
      "Features shape: (50000, 1024), Labels shape: (50000,)\n",
      "[NumPy] Loaded features -> min: -7.8090, max: 7.4968, mean: 0.0354\n",
      "[Torch] A = F^T F / n -> min: -19.8085, max: 21.0236, mean: 0.0013\n",
      "[Torch] F @ A -> min: -12331.8828, max: 11641.4668, mean: 55.0272\n",
      "[NumPy] F @ A (numpy) -> min: -12331.8828, max: 11641.4668, mean: 55.0272\n",
      "\n",
      "[5/18] Processing /scratch/bowenxi/dit/neural_tangent_kernel/feature_swin_b/ca/increment_3_train_tangent.npz\n",
      "\n",
      "Loading: /scratch/bowenxi/dit/neural_tangent_kernel/feature_swin_b/ca/increment_3_train_tangent.npz\n",
      "Features shape: (4353167, 1024), Labels shape: (4353167,)\n",
      "[NumPy] Loaded features -> min: -10.6152, max: 9.6229, mean: 0.0378\n",
      "[Torch] A = F^T F / n -> min: -21.0960, max: 22.1871, mean: 0.0015\n",
      "[Torch] F @ A -> min: -17330.8574, max: 16499.8164, mean: 62.6089\n",
      "[NumPy] F @ A (numpy) -> min: -17330.8574, max: 16499.8164, mean: 62.6098\n",
      "\n",
      "[6/18] Processing /scratch/bowenxi/dit/neural_tangent_kernel/feature_swin_b/ca/increment_3_val_tangent.npz\n",
      "\n",
      "Loading: /scratch/bowenxi/dit/neural_tangent_kernel/feature_swin_b/ca/increment_3_val_tangent.npz\n",
      "Features shape: (50000, 1024), Labels shape: (50000,)\n",
      "[NumPy] Loaded features -> min: -7.7604, max: 7.4966, mean: 0.0362\n",
      "[Torch] A = F^T F / n -> min: -19.8974, max: 20.9974, mean: 0.0014\n",
      "[Torch] F @ A -> min: -12330.0361, max: 11704.4990, mean: 56.4610\n",
      "[NumPy] F @ A (numpy) -> min: -12330.0361, max: 11704.4990, mean: 56.4608\n",
      "\n",
      "[7/18] Processing /scratch/bowenxi/dit/neural_tangent_kernel/feature_swin_b/ca/increment_4_train_tangent.npz\n",
      "\n",
      "Loading: /scratch/bowenxi/dit/neural_tangent_kernel/feature_swin_b/ca/increment_4_train_tangent.npz\n",
      "Features shape: (5377167, 1024), Labels shape: (5377167,)\n",
      "[NumPy] Loaded features -> min: -10.5600, max: 9.6268, mean: 0.0383\n",
      "[Torch] A = F^T F / n -> min: -21.2443, max: 22.2782, mean: 0.0015\n",
      "[Torch] F @ A -> min: -17407.6348, max: 16619.6543, mean: 63.9971\n",
      "[NumPy] F @ A (numpy) -> min: -17407.6348, max: 16619.6543, mean: 63.9958\n",
      "\n",
      "[8/18] Processing /scratch/bowenxi/dit/neural_tangent_kernel/feature_swin_b/ca/increment_4_val_tangent.npz\n",
      "\n",
      "Loading: /scratch/bowenxi/dit/neural_tangent_kernel/feature_swin_b/ca/increment_4_val_tangent.npz\n",
      "Features shape: (50000, 1024), Labels shape: (50000,)\n",
      "[NumPy] Loaded features -> min: -7.7325, max: 7.4952, mean: 0.0366\n",
      "[Torch] A = F^T F / n -> min: -19.9514, max: 20.9865, mean: 0.0014\n",
      "[Torch] F @ A -> min: -12333.6367, max: 11744.6299, mean: 57.3562\n",
      "[NumPy] F @ A (numpy) -> min: -12333.6367, max: 11744.6299, mean: 57.3562\n",
      "\n",
      "[9/18] Processing /scratch/bowenxi/dit/neural_tangent_kernel/feature_swin_b/ca/increment_5_train_tangent.npz\n",
      "\n",
      "Loading: /scratch/bowenxi/dit/neural_tangent_kernel/feature_swin_b/ca/increment_5_train_tangent.npz\n",
      "Features shape: (6401167, 1024), Labels shape: (6401167,)\n",
      "[NumPy] Loaded features -> min: -10.5206, max: 9.6298, mean: 0.0387\n",
      "[Torch] A = F^T F / n -> min: -21.3499, max: 22.3414, mean: 0.0015\n",
      "[Torch] F @ A -> min: -17462.1270, max: 16706.3418, mean: 64.9765\n",
      "[NumPy] F @ A (numpy) -> min: -17462.1270, max: 16706.3418, mean: 64.9757\n",
      "\n",
      "[10/18] Processing /scratch/bowenxi/dit/neural_tangent_kernel/feature_swin_b/ca/increment_5_val_tangent.npz\n",
      "\n",
      "Loading: /scratch/bowenxi/dit/neural_tangent_kernel/feature_swin_b/ca/increment_5_val_tangent.npz\n",
      "Features shape: (50000, 1024), Labels shape: (50000,)\n",
      "[NumPy] Loaded features -> min: -7.7113, max: 7.4939, mean: 0.0369\n",
      "[Torch] A = F^T F / n -> min: -19.9898, max: 20.9769, mean: 0.0014\n",
      "[Torch] F @ A -> min: -12333.5850, max: 11771.8076, mean: 57.9820\n",
      "[NumPy] F @ A (numpy) -> min: -12333.5850, max: 11771.8076, mean: 57.9819\n",
      "\n",
      "[11/18] Processing /scratch/bowenxi/dit/neural_tangent_kernel/feature_swin_b/ca/increment_6_train_tangent.npz\n",
      "\n",
      "Loading: /scratch/bowenxi/dit/neural_tangent_kernel/feature_swin_b/ca/increment_6_train_tangent.npz\n",
      "Features shape: (7425167, 1024), Labels shape: (7425167,)\n",
      "[NumPy] Loaded features -> min: -10.4954, max: 9.6328, mean: 0.0390\n",
      "[Torch] A = F^T F / n -> min: -21.4286, max: 22.3915, mean: 0.0015\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 28.33 GiB. GPU 0 has a total capacity of 79.20 GiB of which 21.65 GiB is free. Including non-PyTorch memory, this process has 57.55 GiB memory in use. Of the allocated memory 56.68 GiB is allocated by PyTorch, and 18.88 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mOutOfMemoryError\u001b[39m                          Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 57\u001b[39m\n\u001b[32m     55\u001b[39m \u001b[38;5;66;03m# ---- Run on Folder ----\u001b[39;00m\n\u001b[32m     56\u001b[39m folder_path = \u001b[33m\"\u001b[39m\u001b[33m/scratch/bowenxi/dit/neural_tangent_kernel/feature_swin_b/ca/\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m57\u001b[39m \u001b[43mprocess_all_npz_files\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfolder_path\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 53\u001b[39m, in \u001b[36mprocess_all_npz_files\u001b[39m\u001b[34m(folder_path)\u001b[39m\n\u001b[32m     51\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m[\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi+\u001b[32m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(npz_files)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m] Processing \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     52\u001b[39m features, labels = load_npz_features(file_path)\n\u001b[32m---> \u001b[39m\u001b[32m53\u001b[39m A, tangent_features = \u001b[43mcompute_tangent_features\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 34\u001b[39m, in \u001b[36mcompute_tangent_features\u001b[39m\u001b[34m(F_np)\u001b[39m\n\u001b[32m     31\u001b[39m A_tensor = torch.matmul(F_T, F_device) / n\n\u001b[32m     32\u001b[39m summarize_tensor_stats(\u001b[33m\"\u001b[39m\u001b[33mA = F^T F / n\u001b[39m\u001b[33m\"\u001b[39m, A_tensor)\n\u001b[32m---> \u001b[39m\u001b[32m34\u001b[39m FA_tensor = \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmatmul\u001b[49m\u001b[43m(\u001b[49m\u001b[43mF_device\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mA_tensor\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     35\u001b[39m summarize_tensor_stats(\u001b[33m\"\u001b[39m\u001b[33mF @ A\u001b[39m\u001b[33m\"\u001b[39m, FA_tensor)\n\u001b[32m     37\u001b[39m FA_np = FA_tensor.cpu().numpy()\n",
      "\u001b[31mOutOfMemoryError\u001b[39m: CUDA out of memory. Tried to allocate 28.33 GiB. GPU 0 has a total capacity of 79.20 GiB of which 21.65 GiB is free. Including non-PyTorch memory, this process has 57.55 GiB memory in use. Of the allocated memory 56.68 GiB is allocated by PyTorch, and 18.88 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "# Tangent Kernel Feature Debugging Notebook\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import os\n",
    "import glob\n",
    "\n",
    "# ---- Utility Functions ----\n",
    "def summarize_numpy_stats(name, array):\n",
    "    print(f\"[NumPy] {name} -> min: {np.min(array):.4f}, max: {np.max(array):.4f}, mean: {np.mean(array):.4f}\")\n",
    "\n",
    "def summarize_tensor_stats(name, tensor):\n",
    "    print(f\"[Torch] {name} -> min: {tensor.min().item():.4f}, max: {tensor.max().item():.4f}, mean: {tensor.mean().item():.4f}\")\n",
    "\n",
    "# ---- Load Real NPZ File ----\n",
    "def load_npz_features(npz_path):\n",
    "    print(f\"\\nLoading: {npz_path}\")\n",
    "    data = np.load(npz_path)\n",
    "    features = data['features']\n",
    "    labels = data['labels']\n",
    "    print(f\"Features shape: {features.shape}, Labels shape: {labels.shape}\")\n",
    "    summarize_numpy_stats(\"Loaded features\", features)\n",
    "    return features, labels\n",
    "\n",
    "# ---- Compute and Check Tangent Kernel ----\n",
    "def compute_tangent_features(F_np):\n",
    "    F_tensor = torch.from_numpy(F_np).float()\n",
    "    F_T = F_tensor.T.to(\"cuda:0\")\n",
    "    F_device = F_tensor.to(\"cuda:0\")\n",
    "    n = F_tensor.shape[0]\n",
    "    A_tensor = torch.matmul(F_T, F_device) / n\n",
    "    summarize_tensor_stats(\"A = F^T F / n\", A_tensor)\n",
    "\n",
    "    FA_tensor = torch.matmul(F_device, A_tensor)\n",
    "    summarize_tensor_stats(\"F @ A\", FA_tensor)\n",
    "\n",
    "    FA_np = FA_tensor.cpu().numpy()\n",
    "    summarize_numpy_stats(\"F @ A (numpy)\", FA_np)\n",
    "    return A_tensor.cpu().numpy(), FA_np\n",
    "\n",
    "# ---- Process All Files in Folder ----\n",
    "def process_all_npz_files(folder_path):\n",
    "    npz_files = sorted(glob.glob(os.path.join(folder_path, \"*.npz\")))\n",
    "    if not npz_files:\n",
    "        print(f\"No .npz files found in {folder_path}\")\n",
    "        return\n",
    "\n",
    "    print(f\"Found {len(npz_files)} .npz files in {folder_path}\")\n",
    "\n",
    "    for i, file_path in enumerate(npz_files):\n",
    "        print(f\"\\n[{i+1}/{len(npz_files)}] Processing {file_path}\")\n",
    "        features, labels = load_npz_features(file_path)\n",
    "        A, tangent_features = compute_tangent_features(features)\n",
    "\n",
    "# ---- Run on Folder ----\n",
    "folder_path = \"/scratch/bowenxi/dit/neural_tangent_kernel/feature_swin_b/ca/\"\n",
    "process_all_npz_files(folder_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8902f9bb-94a6-4e3a-a95e-f89c451dfe47",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DiT",
   "language": "python",
   "name": "dit"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
