{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ba2473d1-3fa4-472c-a209-51b6b94fdce5",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnp\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mh5py\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mnn\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnn\u001b[39;00m\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01moptim\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01moptim\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/DiT/lib/python3.12/site-packages/torch/__init__.py:2129\u001b[39m\n\u001b[32m   2125\u001b[39m sys.modules.setdefault(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.classes\u001b[39m\u001b[33m\"\u001b[39m, classes)\n\u001b[32m   2127\u001b[39m \u001b[38;5;66;03m# quantization depends on torch.fx and torch.ops\u001b[39;00m\n\u001b[32m   2128\u001b[39m \u001b[38;5;66;03m# Import quantization\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m2129\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m quantization \u001b[38;5;28;01mas\u001b[39;00m quantization  \u001b[38;5;66;03m# usort: skip\u001b[39;00m\n\u001b[32m   2131\u001b[39m \u001b[38;5;66;03m# Import the quasi random sampler\u001b[39;00m\n\u001b[32m   2132\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m quasirandom \u001b[38;5;28;01mas\u001b[39;00m quasirandom  \u001b[38;5;66;03m# usort: skip\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/DiT/lib/python3.12/site-packages/torch/quantization/__init__.py:2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# mypy: allow-untyped-defs\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mfake_quantize\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m *  \u001b[38;5;66;03m# noqa: F403\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mfuse_modules\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m fuse_modules\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mfuser_method_mappings\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m *  \u001b[38;5;66;03m# noqa: F403\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/DiT/lib/python3.12/site-packages/torch/quantization/fake_quantize.py:10\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# flake8: noqa: F401\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[33mr\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[33;03mThis file is in the process of migration to `torch/ao/quantization`, and\u001b[39;00m\n\u001b[32m      4\u001b[39m \u001b[33;03mis kept here for compatibility while the migration process is ongoing.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m      7\u001b[39m \u001b[33;03mhere.\u001b[39;00m\n\u001b[32m      8\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mao\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mquantization\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mfake_quantize\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     11\u001b[39m     _is_fake_quant_script_module,\n\u001b[32m     12\u001b[39m     _is_per_channel,\n\u001b[32m     13\u001b[39m     _is_per_tensor,\n\u001b[32m     14\u001b[39m     _is_symmetric_quant,\n\u001b[32m     15\u001b[39m     default_fake_quant,\n\u001b[32m     16\u001b[39m     default_fixed_qparams_range_0to1_fake_quant,\n\u001b[32m     17\u001b[39m     default_fixed_qparams_range_neg1to1_fake_quant,\n\u001b[32m     18\u001b[39m     default_fused_act_fake_quant,\n\u001b[32m     19\u001b[39m     default_fused_per_channel_wt_fake_quant,\n\u001b[32m     20\u001b[39m     default_fused_wt_fake_quant,\n\u001b[32m     21\u001b[39m     default_histogram_fake_quant,\n\u001b[32m     22\u001b[39m     default_per_channel_weight_fake_quant,\n\u001b[32m     23\u001b[39m     default_weight_fake_quant,\n\u001b[32m     24\u001b[39m     disable_fake_quant,\n\u001b[32m     25\u001b[39m     disable_observer,\n\u001b[32m     26\u001b[39m     enable_fake_quant,\n\u001b[32m     27\u001b[39m     enable_observer,\n\u001b[32m     28\u001b[39m     FakeQuantize,\n\u001b[32m     29\u001b[39m     FakeQuantizeBase,\n\u001b[32m     30\u001b[39m     FixedQParamsFakeQuantize,\n\u001b[32m     31\u001b[39m     FusedMovingAvgObsFakeQuantize,\n\u001b[32m     32\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/DiT/lib/python3.12/site-packages/torch/ao/quantization/__init__.py:28\u001b[39m\n\u001b[32m     26\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mqconfig_mapping\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m *  \u001b[38;5;66;03m# noqa: F403\u001b[39;00m\n\u001b[32m     27\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mquant_type\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m *  \u001b[38;5;66;03m# noqa: F403\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m28\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mquantization_mappings\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m *  \u001b[38;5;66;03m# noqa: F403 # type: ignore[no-redef]\u001b[39;00m\n\u001b[32m     29\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mquantize\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m *  \u001b[38;5;66;03m# noqa: F403\u001b[39;00m\n\u001b[32m     30\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mquantize_jit\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m *  \u001b[38;5;66;03m# noqa: F403\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/DiT/lib/python3.12/site-packages/torch/ao/quantization/quantization_mappings.py:18\u001b[39m\n\u001b[32m     14\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mao\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mnn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mquantized\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mreference\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnnqr\u001b[39;00m\n\u001b[32m     16\u001b[39m \u001b[38;5;66;03m# Because `torch.ao.nn` uses lazy imports, we need to make\u001b[39;00m\n\u001b[32m     17\u001b[39m \u001b[38;5;66;03m# sure we import the contents explicitly here.\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m18\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mao\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mnn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01msparse\u001b[39;00m\n\u001b[32m     19\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mnn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mfunctional\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mF\u001b[39;00m\n\u001b[32m     20\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m nn\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/DiT/lib/python3.12/site-packages/torch/ao/nn/sparse/__init__.py:1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m quantized\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/DiT/lib/python3.12/site-packages/torch/ao/nn/sparse/quantized/__init__.py:1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mao\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mnn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01msparse\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mquantized\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m dynamic\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mlinear\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Linear, LinearPackedParams\n\u001b[32m      6\u001b[39m __all__ = [\n\u001b[32m      7\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mdynamic\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m      8\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mLinear\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m      9\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mLinearPackedParams\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     10\u001b[39m ]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/DiT/lib/python3.12/site-packages/torch/ao/nn/sparse/quantized/dynamic/__init__.py:1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mlinear\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Linear\n\u001b[32m      4\u001b[39m __all__ = [\n\u001b[32m      5\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mLinear\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m      6\u001b[39m ]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap>:1360\u001b[39m, in \u001b[36m_find_and_load\u001b[39m\u001b[34m(name, import_)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap>:1322\u001b[39m, in \u001b[36m_find_and_load_unlocked\u001b[39m\u001b[34m(name, import_)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap>:1262\u001b[39m, in \u001b[36m_find_spec\u001b[39m\u001b[34m(name, path, target)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap_external>:1532\u001b[39m, in \u001b[36mfind_spec\u001b[39m\u001b[34m(cls, fullname, path, target)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap_external>:1506\u001b[39m, in \u001b[36m_get_spec\u001b[39m\u001b[34m(cls, fullname, path, target)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap_external>:1638\u001b[39m, in \u001b[36mfind_spec\u001b[39m\u001b[34m(self, fullname, target)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap_external>:161\u001b[39m, in \u001b[36m_path_isfile\u001b[39m\u001b[34m(path)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap_external>:153\u001b[39m, in \u001b[36m_path_is_mode_type\u001b[39m\u001b[34m(path, mode)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap_external>:147\u001b[39m, in \u001b[36m_path_stat\u001b[39m\u001b[34m(path)\u001b[39m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import glob\n",
    "\n",
    "# Configuration\n",
    "REAL_TRAIN_DATA_PATH = \"/data/yyang409/bowen/imagenet_feature/swin_base/patch4_window7_224/image_features_w_label_train.npz\"\n",
    "REAL_VAL_DATA_PATH = \"/data/yyang409/bowen/imagenet_feature/swin_base/patch4_window7_224/image_features_w_label_val.npz\"\n",
    "SYNTH_DATA_DIR = \"/scratch/bowenxi/dit/data_gen/B_4/final_data/\"  # Directory containing the NPZ files\n",
    "SYNTH_DATA_PATTERN = \"imagenet_latents_*.npz\"  # Pattern to match your NPZ files\n",
    "NUM_CLASSES = 1000\n",
    "SAMPLES_PER_CLASS = 1024  # Number of synthetic samples per class in each increment\n",
    "MAX_MULTIPLES = 10  # Maximum number of multiples to test (10x)\n",
    "BATCH_SIZE = 1024\n",
    "EPOCHS = 50\n",
    "\n",
    "def load_and_prepare_data():\n",
    "    \"\"\"Load and preprocess real/synthetic features\"\"\"\n",
    "    # Load real training data\n",
    "    print(\"Loading real training data...\")\n",
    "    with np.load(REAL_TRAIN_DATA_PATH) as data:\n",
    "        real_train_features = data['features']\n",
    "        real_train_labels = data['labels']\n",
    "    \n",
    "    # Load real validation data\n",
    "    print(\"Loading real validation data...\")\n",
    "    with np.load(REAL_VAL_DATA_PATH) as data:\n",
    "        real_val_features = data['features']\n",
    "        real_val_labels = data['labels']\n",
    "    \n",
    "    # Load synthetic data from multiple NPZ files\n",
    "    print(\"Loading synthetic data from multiple NPZ files...\")\n",
    "    synth_features_list = []\n",
    "    synth_labels_list = []\n",
    "    \n",
    "    # Get list of all NPZ files matching the pattern\n",
    "    npz_files = sorted(glob.glob(os.path.join(SYNTH_DATA_DIR, SYNTH_DATA_PATTERN)))\n",
    "    print(f\"Found {len(npz_files)} synthetic data files: {[os.path.basename(f) for f in npz_files]}\")\n",
    "    \n",
    "    # Load each NPZ file and append to our lists\n",
    "    for npz_file in npz_files:\n",
    "        print(f\"Loading {os.path.basename(npz_file)}...\")\n",
    "        with np.load(npz_file) as data:\n",
    "            # Assuming each NPZ file has 'features' and 'labels' keys - adjust as needed\n",
    "            features = data['samples']\n",
    "            labels = data['labels']\n",
    "            \n",
    "            synth_features_list.append(features)\n",
    "            synth_labels_list.append(labels)\n",
    "    \n",
    "    # Combine all synthetic data\n",
    "    synth_features = np.vstack(synth_features_list)\n",
    "    synth_labels = np.concatenate(synth_labels_list)\n",
    "    \n",
    "    # Check for NaN values in synthetic data\n",
    "    if np.isnan(synth_features).any():\n",
    "        print(f\"WARNING: Found {np.isnan(synth_features).sum()} NaN values in synthetic features\")\n",
    "        print(\"Replacing NaN values with 0...\")\n",
    "        synth_features = np.nan_to_num(synth_features, nan=0.0)\n",
    "    \n",
    "    # Print dataset statistics\n",
    "    print(f\"Real training data: {real_train_features.shape[0]} samples\")\n",
    "    print(f\"Real validation data: {real_val_features.shape[0]} samples\")\n",
    "    print(f\"Synthetic data: {synth_features.shape[0]} samples\")\n",
    "    \n",
    "    # Verify label distributions\n",
    "    real_class_counts = np.bincount(real_train_labels, minlength=NUM_CLASSES)\n",
    "    synth_class_counts = np.bincount(synth_labels, minlength=NUM_CLASSES)\n",
    "    \n",
    "    print(f\"Real data class distribution: min={real_class_counts.min()}, max={real_class_counts.max()}, avg={real_class_counts.mean():.1f}\")\n",
    "    print(f\"Synthetic data class distribution: min={synth_class_counts.min()}, max={synth_class_counts.max()}, avg={synth_class_counts.mean():.1f}\")\n",
    "    \n",
    "    # Standardize features using training data statistics\n",
    "    print(\"Standardizing features...\")\n",
    "    mean = np.mean(real_train_features, axis=0)\n",
    "    std = np.std(real_train_features, axis=0) + 1e-8\n",
    "    \n",
    "    # Apply standardization to all datasets\n",
    "    real_train_features = (real_train_features - mean) / std\n",
    "    real_val_features = (real_val_features - mean) / std\n",
    "    synth_features = (synth_features - mean) / std\n",
    "    \n",
    "    return (real_train_features, real_train_labels,\n",
    "            real_val_features, real_val_labels,\n",
    "            synth_features, synth_labels)\n",
    "\n",
    "def prepare_synthetic_data_by_ratio(synth_features, synth_labels, ratio):\n",
    "    \"\"\"Prepare synthetic data for a given ratio - take samples per class based on ratio\"\"\"\n",
    "    samples_to_take = SAMPLES_PER_CLASS * ratio\n",
    "    \n",
    "    # Initialize arrays to store selected data\n",
    "    selected_features = []\n",
    "    selected_labels = []\n",
    "    \n",
    "    # For each class, take the appropriate number of samples\n",
    "    for class_idx in range(NUM_CLASSES):\n",
    "        # Find indices for this class\n",
    "        class_indices = np.where(synth_labels == class_idx)[0]\n",
    "        \n",
    "        # If we have enough samples, take the first N\n",
    "        if len(class_indices) >= samples_to_take:\n",
    "            idx_to_use = class_indices[:samples_to_take]\n",
    "        else:\n",
    "            # If not enough samples, take all available and repeat as needed\n",
    "            idx_to_use = np.concatenate([class_indices] * (samples_to_take // len(class_indices) + 1))[:samples_to_take]\n",
    "        \n",
    "        # Add selected samples to our arrays\n",
    "        selected_features.append(synth_features[idx_to_use])\n",
    "        selected_labels.append(synth_labels[idx_to_use])\n",
    "    \n",
    "    # Combine all selected samples\n",
    "    X_synth = np.vstack(selected_features)\n",
    "    y_synth = np.concatenate(selected_labels)\n",
    "    \n",
    "    return X_synth, y_synth\n",
    "\n",
    "class AdaptiveMLP(nn.Module):\n",
    "    def __init__(self, input_dim, num_classes):\n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(input_dim, 1024),\n",
    "            nn.BatchNorm1d(1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            \n",
    "            nn.Linear(1024, 1024),\n",
    "            nn.BatchNorm1d(1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            \n",
    "            nn.Linear(1024, num_classes)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n",
    "\n",
    "def train_model(X_train, y_train, X_val, y_val, ratio):\n",
    "    \"\"\"Train and evaluate MLP\"\"\"\n",
    "    # Convert to PyTorch tensors\n",
    "    train_dataset = TensorDataset(\n",
    "        torch.FloatTensor(X_train),\n",
    "        torch.LongTensor(y_train)\n",
    "    )\n",
    "    val_dataset = TensorDataset(\n",
    "        torch.FloatTensor(X_val),\n",
    "        torch.LongTensor(y_val)\n",
    "    )\n",
    "    \n",
    "    # Create loaders\n",
    "    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE)\n",
    "    \n",
    "    # Initialize model\n",
    "    model = AdaptiveMLP(X_train.shape[1], NUM_CLASSES)\n",
    "    model = model.cuda()\n",
    "    \n",
    "    # Training setup\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=2e-3)\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'max', patience=3)\n",
    "    \n",
    "    best_acc = 0\n",
    "    for epoch in range(EPOCHS):\n",
    "        # Training\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        \n",
    "        for inputs, labels in tqdm(train_loader, desc=f\"Epoch {epoch+1}\"):\n",
    "            inputs, labels = inputs.cuda(), labels.cuda()\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += labels.size(0)\n",
    "            correct += predicted.eq(labels).sum().item()\n",
    "        \n",
    "        train_acc = correct / total\n",
    "        print(f\"Epoch {epoch+1}: Train Loss: {train_loss/len(train_loader):.4f} | Train Acc: {train_acc:.2%}\")\n",
    "        \n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in val_loader:\n",
    "                inputs, labels = inputs.cuda(), labels.cuda()\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "                \n",
    "                val_loss += loss.item()\n",
    "                _, predicted = outputs.max(1)\n",
    "                total += labels.size(0)\n",
    "                correct += predicted.eq(labels).sum().item()\n",
    "        \n",
    "        val_acc = correct / total\n",
    "        print(f\"Epoch {epoch+1}: Val Loss: {val_loss/len(val_loader):.4f} | Val Acc: {val_acc:.2%}\")\n",
    "        \n",
    "        scheduler.step(val_acc)\n",
    "        \n",
    "        # Save best model\n",
    "        if val_acc > best_acc:\n",
    "            best_acc = val_acc\n",
    "            torch.save(model.state_dict(), f\"best_model_ratio_{ratio}x.pt\")\n",
    "            print(f\"Best model saved with validation accuracy: {val_acc:.2%}\")\n",
    "    \n",
    "    return best_acc\n",
    "\n",
    "def run_experiment():\n",
    "    # Load data\n",
    "    (real_train_features, real_train_labels,\n",
    "     real_val_features, real_val_labels,\n",
    "     synth_features, synth_labels) = load_and_prepare_data()\n",
    "    \n",
    "    # Track results\n",
    "    results = {}\n",
    "    \n",
    "    # First, train with real data only (0x synthetic)\n",
    "    print(\"\\n=== Training with real data only (0x synthetic) ===\")\n",
    "    val_acc = train_model(real_train_features, real_train_labels, \n",
    "                          real_val_features, real_val_labels, 0)\n",
    "    results[0] = val_acc\n",
    "    print(f\"Real data only | Final Val Acc: {val_acc:.2%}\")\n",
    "    \n",
    "    # Calculate the maximum ratio we can support with the available synthetic data\n",
    "    # We have 3 files, each with 1024 samples per class\n",
    "    available_samples_per_class = 10 * 1024  # 3 files x 1024 samples per class\n",
    "    max_ratio = min(MAX_MULTIPLES, available_samples_per_class // SAMPLES_PER_CLASS)\n",
    "    \n",
    "    print(f\"\\nBased on available synthetic data, we can support up to {max_ratio}x ratios\")\n",
    "    \n",
    "    # Then progressively add more synthetic data\n",
    "    for ratio in range(1, max_ratio + 1):\n",
    "        print(f\"\\n=== Training with {ratio}x synthetic data ===\")\n",
    "        \n",
    "        # Prepare synthetic data for this ratio (SAMPLES_PER_CLASS * ratio per class)\n",
    "        X_synth, y_synth = prepare_synthetic_data_by_ratio(synth_features, synth_labels, ratio)\n",
    "        \n",
    "        print(f\"Using {len(X_synth)} synthetic samples ({len(X_synth)//NUM_CLASSES} per class)\")\n",
    "        \n",
    "        # Combine real and synthetic data\n",
    "        X_train = np.concatenate([real_train_features, X_synth])\n",
    "        y_train = np.concatenate([real_train_labels, y_synth])\n",
    "        \n",
    "        # Train and evaluate\n",
    "        val_acc = train_model(X_train, y_train, real_val_features, real_val_labels, ratio)\n",
    "        results[ratio] = val_acc\n",
    "        print(f\"Ratio {ratio}x | Final Val Acc: {val_acc:.2%}\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "def analyze_results(results):\n",
    "    import matplotlib.pyplot as plt\n",
    "    \n",
    "    ratios = sorted(results.keys())\n",
    "    accuracies = [results[r] for r in ratios]\n",
    "    \n",
    "    plt.figure(figsize=(12, 7))\n",
    "    plt.plot(ratios, accuracies, 'bo-', markersize=8)\n",
    "    plt.xlabel('Synthetic Data Ratio (x real data size per class)', fontsize=14)\n",
    "    plt.ylabel('Validation Accuracy', fontsize=14)\n",
    "    plt.title('ImageNet Classification Accuracy vs Synthetic Data Ratio', fontsize=16)\n",
    "    plt.grid(True)\n",
    "    plt.xticks(ratios)\n",
    "    \n",
    "    # Add text labels for each point\n",
    "    for x, y in zip(ratios, accuracies):\n",
    "        plt.annotate(f\"{y:.2%}\", \n",
    "                    (x, y), \n",
    "                    textcoords=\"offset points\",\n",
    "                    xytext=(0,10), \n",
    "                    ha='center')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('synthetic_data_impact.png', dpi=300)\n",
    "    plt.show()\n",
    "    \n",
    "    # Print numeric results for reference\n",
    "    print(\"\\nNumeric Results:\")\n",
    "    print(\"Ratio | Validation Accuracy\")\n",
    "    print(\"-\" * 30)\n",
    "    for r, acc in zip(ratios, accuracies):\n",
    "        print(f\"{r}x    | {acc:.4f} ({acc:.2%})\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    experiment_results = run_experiment()\n",
    "    analyze_results(experiment_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "426643bc-091d-4a05-abb1-57856b44a58f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DiT",
   "language": "python",
   "name": "dit"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
